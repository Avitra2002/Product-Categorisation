{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TRANSFORMING SURVERY/PROBLEM SOLUTION\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from Functions import is_english, format_date, is_valid_feedback\n",
    "import google.cloud.logging\n",
    "from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "\n",
    "# Initialize Cloud Logging\n",
    "client = google.cloud.logging.Client()\n",
    "handler = CloudLoggingHandler(client)\n",
    "cloud_logger = logging.getLogger('cloudLogger')\n",
    "cloud_logger.setLevel(logging.DEBUG)\n",
    "cloud_logger.addHandler(handler)\n",
    "\n",
    "\n",
    "def process_survey_data(product, source, file_path):\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        cloud_logger.info(\"Data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        cloud_logger.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "    date_column = None\n",
    "    for col in data.columns:\n",
    "        if 'date' in col.lower() or data[col].apply(lambda x: isinstance(x, str) and '/' in x).all():\n",
    "            date_column = col\n",
    "            data[col] = data[col].apply(format_date)\n",
    "            cloud_logger.info(f\"Date column identified: {col}\")\n",
    "            break\n",
    "\n",
    "    if not date_column:\n",
    "        cloud_logger.warning(\"No date column identified.\")\n",
    "\n",
    "    question_texts = data.iloc[0]\n",
    "\n",
    "    columns_to_drop = []\n",
    "    for col in data.columns:\n",
    "        if not (col.startswith('Q') and col[1:].isdigit()):\n",
    "            if col != date_column:\n",
    "                columns_to_drop.append(col)\n",
    "        else:\n",
    "            if \"NPS\" in col or \"rating\" in col.lower() or \"scale\" in col.lower():\n",
    "                columns_to_drop.append(col)\n",
    "            elif pd.to_numeric(data[col][1:], errors='coerce').notna().all():\n",
    "                columns_to_drop.append(col)\n",
    "            elif data[col][1:].str.strip().str.lower().isin(['yes', 'no']).all():\n",
    "                columns_to_drop.append(col)\n",
    "\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "    # cloud_logger.info(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "    data = data[1:]\n",
    "\n",
    "    data = data[~data.apply(lambda row: row.astype(str).str.contains('QID').any(), axis=1)]\n",
    "    # cloud_logger.info(\"Filtered out rows containing 'QID'.\")\n",
    "\n",
    "\n",
    "    for col in data.columns:\n",
    "        data[col] = data[col].apply(lambda x: x if is_valid_feedback(x) and is_english(x) else None)\n",
    "        # cloud_logger.info(f\"Processed feedback for column: {col}\")\n",
    "\n",
    "    for col in data.columns:\n",
    "        if col.startswith('Q'):\n",
    "            question_text = question_texts[col]\n",
    "            data[col] = data[col].apply(lambda x: f\"{source}: {question_text}: {x}\" if pd.notna(x) else x)\n",
    "            # cloud_logger.info(f\"Appended question text to column: {col}\")\n",
    "\n",
    "    data.dropna(axis=1, how='all', inplace=True)\n",
    "    # cloud_logger.info(\"Dropped columns with all NaN values.\")\n",
    "\n",
    "    long_format_data = data.melt(var_name='Question Code', value_name='Feedback')\n",
    "    long_format_data = long_format_data.dropna(subset=['Feedback'])\n",
    "    # cloud_logger.info(\"Converted data to long format and filtered out rows with NaN feedback.\")\n",
    "\n",
    "    if product != \"Others\":\n",
    "        long_format_data['Product'] = product\n",
    "        long_format_data['Subcategory'] = None\n",
    "        long_format_data['Feedback Category'] = ''\n",
    "        long_format_data['Sentiment'] = None\n",
    "        long_format_data['Sentiment Score'] = None\n",
    "        long_format_data['Source'] = source\n",
    "\n",
    "        #TODO: Classification\n",
    "\n",
    "\n",
    "    \n",
    "    else:\n",
    "        long_format_data['Product'] = None\n",
    "        long_format_data['Subcategory'] = None\n",
    "        long_format_data['Feedback Category'] = ''\n",
    "        long_format_data['Sentiment'] = None\n",
    "        long_format_data['Sentiment Score'] = None\n",
    "        long_format_data['Source'] = source\n",
    "\n",
    "        #TODO: Classification\n",
    "\n",
    "    if date_column:\n",
    "        long_format_data['Date'] = data[date_column].values\n",
    "\n",
    "    desired_columns = ['Date', 'Feedback', 'Product', 'Subcategory', 'Feedback Category', 'Sentiment', 'Sentiment Score', 'Source']\n",
    "    long_format_data = long_format_data.reindex(columns=desired_columns)\n",
    "\n",
    "    \n",
    "\n",
    "    #TODO: Add to SQL Analytics\n",
    "\n",
    "    # output_file_path = f'/path/to/output/Transformed_{product}_{source}.csv'\n",
    "    # try:\n",
    "    #     long_format_data.to_csv(output_file_path, index=False)\n",
    "    #     cloud_logger.info(f\"Data transformation complete. File saved to: {output_file_path}\")\n",
    "    # except Exception as e:\n",
    "    #     cloud_logger.error(f\"Error saving transformed data: {e}\")\n",
    "    #     raise\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
